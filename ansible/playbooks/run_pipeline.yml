- name: Run Data Analysis Pipeline
  hosts: host
  become: yes
  vars:
    datasets:
      - organism: "human"
        data_input_dir: "/mnt/datasets/human_proteome/"
        results_dir: "/mnt/results/human/"
      - organism: "ecoli"
        data_input_dir: "/mnt/datasets/ecoli_proteome/"
        results_dir: "/mnt/results/ecoli/"
    redis_host: "{{ hostvars['storage']['ansible_host'] }}"
    celery_user: "almalinux"
    celery_group: "almalinux"
    virtualenv_path: "/opt/merizo_search/merizosearch_env"
    dispatch_script: "/opt/data_pipeline/dispatch_tasks.py"
    worker_queues:
      worker1: "worker1_queue"
      worker2: "worker2_queue"
      worker3: "worker3_queue"
  tasks:
    - name: Find all .pdb files for each dataset
      find:
        paths: "{{ item.data_input_dir }}"
        patterns: "*.pdb"
      register: pdb_files
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"

    - name: Set fact for datasets and their PDB files
      set_fact:
        datasets_pdb: "{{ datasets_pdb | default([]) + [ {
          'organism': item.item.organism,
          'results_dir': item.item.results_dir,
          'data_input_dir': item.item.data_input_dir,
          'files': item.files | map(attribute='path') | list
        } ] }}"
      loop: "{{ pdb_files.results }}"
      loop_control:
        label: "{{ item.item.organism }}"

    # **Moved Dispatch and Cleanup Tasks Start Here**

    - name: Deploy Updated Dispatch Tasks Script on host
      copy:
        dest: /opt/data_pipeline/dispatch_tasks.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          import sys
          import redis
          from celery import Celery
          import glob
          import os
          import logging

          # Configure logging
          logging.basicConfig(
              filename='/opt/data_pipeline/dispatch_tasks.log',
              level=logging.DEBUG,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )

          redis_host = "{{ redis_host }}"
          redis_port = 6379
          redis_db = 0

          # Define worker queues with actual worker names
          WORKER_QUEUES = {
          {% for w, q in worker_queues.items() %}
              "{{ w }}": "{{ q }}"{% if not loop.last %},{% endif %}
          {% endfor %}
          }

          app = Celery('celery_worker', broker='redis://{}:{}/0'.format(redis_host, redis_port))

          def get_enabled_workers():
              r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
              disabled = r.smembers('disabled_workers')
              disabled = {d.decode('utf-8') for d in disabled}
              enabled = {w: q for w, q in WORKER_QUEUES.items() if w not in disabled}
              logging.debug(f"Disabled workers: {disabled}")
              logging.debug(f"Enabled workers: {enabled}")
              return enabled

          def main(input_dir, output_dir, organism):
              if organism not in ["human", "ecoli", "test"]:
                  print("Error: ORGANISM must be either 'human', 'ecoli', or 'test'")
                  sys.exit(1)

              # Initialize Redis connection
              r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
              dispatched_set_key = f"dispatched_tasks:{organism}"

              while True:
                  enabled_workers = get_enabled_workers()
                  if not enabled_workers:
                      print("No enabled workers available. Check CPU load or alerts.")
                      sys.exit(1)

                  pdb_files = glob.glob(os.path.join(input_dir, "*.pdb"))
                  pdb_files_to_process = [
                      f for f in pdb_files
                      if not os.path.exists(os.path.join(output_dir, f"{os.path.splitext(os.path.basename(f))[0]}.parsed"))
                      and not r.sismember(dispatched_set_key, f)
                  ][:100]  # Batch size of 100

                  if not pdb_files_to_process:
                      print(f"No new .pdb files to process for {organism}.")
                      break

                  worker_list = list(enabled_workers.items())
                  worker_count = len(worker_list)
                  task_index = 0

                  for pdb_file in pdb_files_to_process:
                      worker, queue = worker_list[task_index % worker_count]
                      result = app.send_task(
                          'celery_worker.run_pipeline',
                          args=[pdb_file, output_dir, organism],
                          queue=queue
                      )
                      logging.info(f"Task {result.id} dispatched for {pdb_file} to '{queue}' queue.")
                      print(f"Task {result.id} dispatched for {pdb_file} to '{queue}' queue.")
                      r.sadd(dispatched_set_key, pdb_file)
                      task_index += 1

          if __name__ == "__main__":
              if len(sys.argv) != 4:
                  print("Usage: python3 dispatch_tasks.py [INPUT_DIR] [OUTPUT_DIR] [ORGANISM]")
                  sys.exit(1)
              input_dir = sys.argv[1]
              output_dir = sys.argv[2]
              organism = sys.argv[3].lower()
              main(input_dir, output_dir, organism)

    - name: Create dispatch_tasks.sh Wrapper Script
      copy:
        dest: /opt/data_pipeline/dispatch_tasks.sh
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          #!/bin/bash
          source {{ virtualenv_path }}/bin/activate
          exec python3 {{ dispatch_script }} "$@"

    - name: Deploy Dispatch Tasks systemd Service Files for Each Dataset
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"
      copy:
        dest: "/etc/systemd/system/dispatch_tasks_{{ item.organism }}.service"
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Dispatch Tasks Service for {{ item.organism }}
          After=network.target
          Wants=network.target

          [Service]
          Type=simple
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/opt/data_pipeline/dispatch_tasks.sh "{{ item.data_input_dir }}" "{{ item.results_dir }}" "{{ item.organism }}"
          Restart=always
          RestartSec=5s

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start Dispatch Tasks services for Each Dataset
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"
      systemd:
        name: "dispatch_tasks_{{ item.organism }}.service"
        state: started
        enabled: yes

    - name: Deploy Redis cleanup Python script
      copy:
        dest: /usr/local/bin/redis_cleanup.py
        owner: root
        group: root
        mode: '0755'
        content: |
          #!/usr/bin/env python3
          import redis

          REDIS_HOST = "{{ redis_host }}"
          REDIS_PORT = 6379
          REDIS_DB   = 0

          r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=redis_DB)
          r.delete('dispatched_tasks:human')
          r.delete('dispatched_tasks:ecoli')

    - name: Deploy redis_task_cleanup.service
      copy:
        dest: /etc/systemd/system/redis_task_cleanup.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Remove dispatched_tasks:human and dispatched_tasks:ecoli from Redis

          [Service]
          Type=oneshot
          ExecStart=/usr/bin/python3 /usr/local/bin/redis_cleanup.py

    - name: Deploy redis_task_cleanup.timer
      copy:
        dest: /etc/systemd/system/redis_task_cleanup.timer
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Timer for redis_task_cleanup.service

          [Timer]
          OnCalendar=hourly
          Persistent=true
          Unit=redis_task_cleanup.service

          [Install]
          WantedBy=timers.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start redis_task_cleanup.timer
      systemd:
        name: redis_task_cleanup.timer
        state: started
        enabled: yes

    - name: Enqueue pipeline tasks for each dataset
      debug:
        msg: "Dispatching tasks for organism: {{ item.organism }}"
      loop: "{{ datasets_pdb }}"
      loop_control:
        label: "{{ item.organism }}"
