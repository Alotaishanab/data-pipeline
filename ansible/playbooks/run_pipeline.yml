# Distribute and Run Data Analysis Pipeline via Celery
- name: Distribute and Run Data Analysis Pipeline via Celery
  hosts: host
  become: yes
  vars:
    batch_size: 100
    datasets:
      # Commenting out existing datasets
      # - organism: "human"
      #   data_input_dir: "/mnt/datasets/human_proteome/"
      #   results_dir: "/mnt/results/human/"
      # - organism: "ecoli"
      #   data_input_dir: "/mnt/datasets/ecoli_proteome/"
      #   results_dir: "/mnt/results/ecoli/"
      
      # Adding the new test dataset
      - organism: "test"
        data_input_dir: "/mnt/datasets/test/"
        results_dir: "/mnt/results/test/"
  tasks:
    - name: Ensure results directory exists
      file:
        path: "{{ item.results_dir }}"
        state: directory
        owner: "{{ celery_user }}"       # Replace with appropriate user if different
        group: "{{ celery_group }}"       # Replace with appropriate group if different
        mode: '0755'
      loop: "{{ datasets }}"
    
    - name: Find all .pdb files for each dataset
      find:
        paths: "{{ item.data_input_dir }}"
        patterns: "*.pdb"
      register: pdb_files
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"

    - name: Set fact for datasets and their PDB files
      set_fact:
        datasets_pdb: "{{ datasets_pdb | default([]) + [ {
          'organism': item.item.organism,
          'results_dir': item.item.results_dir,
          'data_input_dir': item.item.data_input_dir,
          'files': item.files | map(attribute='path') | list
        } ] }}"
      loop: "{{ pdb_files.results }}"
      loop_control:
        label: "{{ item.item.organism }}"

    # Limit each dataset to the first 100 PDB files (adjust batch_size if needed)
    - name: Limit each dataset to batch_size .pdb files
      set_fact:
        datasets_pdb_limited: "{{ datasets_pdb_limited | default([]) + [ {
          'organism': item.organism,
          'results_dir': item.results_dir,
          'data_input_dir': item.data_input_dir,
          'files': (item.files | list)[0:batch_size]
        } ] }}"
      loop: "{{ datasets_pdb }}"

    - name: Enqueue pipeline tasks for each dataset
      shell: |
        python3 /opt/data_pipeline/dispatch_tasks.py "{{ item.data_input_dir }}" "{{ item.results_dir }}" "{{ item.organism }}"
      loop: "{{ datasets_pdb_limited }}"
      loop_control:
        label: "{{ item.organism }}"
      when: item.files | length > 0
