- name: Setup Celery Workers (including aggregator on one worker)
  hosts: workers
  become: yes
  vars:
    redis_host: "{{ hostvars['storage']['ansible_host'] }}"
    celery_user: "almalinux"
    celery_group: "almalinux"
    virtualenv_path: "/opt/merizo_search/merizosearch_env"
    celery_bin: "{{ virtualenv_path }}/bin/celery"
    worker_queues:
      worker1: "worker1_queue"
      worker2: "worker2_queue"
      worker3: "worker3_queue"
  tasks:
    - name: Install Celery and Redis Python packages in virtualenv
      pip:
        name:
          - celery
          - redis
        virtualenv: "{{ virtualenv_path }}"
        state: present

    - name: Create data pipeline directory
      file:
        path: /opt/data_pipeline/
        state: directory
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'

    - name: Deploy Celery Worker Script
      copy:
        dest: /opt/data_pipeline/celery_worker.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          import logging
          from celery import Celery
          import subprocess
          import os

          # Configure logging
          logging.basicConfig(
              filename='/opt/data_pipeline/celery_worker.log',
              level=logging.INFO,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )

          # Define the Redis broker URL
          app = Celery('celery_worker', broker='redis://{{ redis_host }}:6379/0')

          @app.task
          def run_pipeline(pdb_file, output_dir, organism):
              """
              Celery task to run the data pipeline on a specified PDB file.
              """
              print(f"Received PDB File: {pdb_file}")
              pipeline_script = "/opt/data_pipeline/pipeline_script.py"
              cmd = [
                  "/opt/merizo_search/merizosearch_env/bin/python3",
                  pipeline_script,
                  pdb_file,
                  output_dir,
                  organism
              ]
              logging.info(f"Running pipeline script: {' '.join(cmd)}")
              try:
                  process = subprocess.run(cmd, capture_output=True, text=True, check=True)
                  logging.info(f"Pipeline STDOUT: {process.stdout}")
                  logging.info(f"Pipeline STDERR: {process.stderr}")
                  return {
                      'stdout': process.stdout,
                      'stderr': process.stderr,
                      'returncode': process.returncode
                  }
              except subprocess.CalledProcessError as e:
                  logging.error(f"Pipeline encountered an error: {e.stderr}")
                  return {
                      'stdout': e.stdout,
                      'stderr': e.stderr,
                      'returncode': e.returncode
                  }

    - name: Set worker name
      set_fact:
        worker_name: "{{ inventory_hostname.split('-')[0] }}"

    - name: Create Celery Startup Shell Script
      copy:
        dest: /opt/data_pipeline/start_celery.sh
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          #!/bin/bash
          source {{ virtualenv_path }}/bin/activate
          exec {{ celery_bin }} -A celery_worker worker --loglevel=info --concurrency=4 --queues={{ worker_queues[worker_name] }} -n {{ worker_name }}

    - name: Deploy Celery Worker systemd Service File
      copy:
        dest: /etc/systemd/system/celery.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Celery Service
          After=network.target

          [Service]
          Type=simple
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/opt/data_pipeline/start_celery.sh
          Restart=always

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Restart and enable Celery service
      systemd:
        name: celery
        state: restarted
        enabled: yes

    - name: Ensure Celery worker script is executable
      file:
        path: /opt/data_pipeline/celery_worker.py
        mode: '0755'


    - name: Deploy aggregator_after_all Python script
      when: "'worker1' in inventory_hostname"
      copy:
        dest: /opt/data_pipeline/aggregator_after_all.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
        #!/usr/bin/env python3
        import logging
        import sys
        import os
        import glob
        import csv
        import statistics
        from collections import defaultdict
        import subprocess

        # Log file configuration
        LOGFILE = "/opt/data_pipeline/aggregator_after_all.log"
        logging.basicConfig(
            filename=LOGFILE,
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )

        # Python binary for the virtual environment
        PYTHON_BIN = "/opt/merizo_search/merizosearch_env/bin/python3"

        # Supported organisms
        ORGANISMS = ['human', 'ecoli']

        def celery_has_active_tasks() -> bool:
            """
            Use Celery inspection to determine if tasks are running.
            """
            try:
                cmd = [
                    PYTHON_BIN, "-m", "celery",
                    "-A", "celery_worker",
                    "inspect", "active"
                ]
                result = subprocess.run(cmd, capture_output=True, text=True, check=False)
                stdout = result.stdout.lower()
                # If "run_pipeline" is present, assume tasks are active
                return ("run_pipeline" in stdout)
            except Exception as e:
                logging.error(f"Failed to inspect Celery tasks: {e}")
                return True

        def aggregate_parsed_files(input_dir, organism):
            """
            Aggregate data from existing parsed files within the given input directory.
            
            It:
              - Reads pLDDT values from the first line of each .parsed file.
              - Aggregates CATH count data from the subsequent rows.
              - Returns a tuple (overall_mean, overall_std, cath_counts).
            """
            logging.info(f"Aggregating results for {organism} from parsed files in {input_dir}...")
            
            plDDT_values = []
            cath_counts = defaultdict(int)
            
            # Find all .parsed files in the input directory (non-recursive)
            parsed_files = glob.glob(os.path.join(input_dir, "*.parsed"))
            logging.info(f"Found {len(parsed_files)} parsed files for {organism}.")
            
            for parsed_file in parsed_files:
                try:
                    with open(parsed_file, "r") as fh:
                        # Expect first line in the format:
                        # "#<search_filename> Results. mean plddt: <value>"
                        first_line = fh.readline().strip()
                        if first_line.startswith("#"):
                            parts = first_line.split("mean plddt:")
                            if len(parts) == 2:
                                try:
                                    value = float(parts[1])
                                    plDDT_values.append(value)
                                except ValueError:
                                    logging.warning(f"Invalid pLDDT value in {parsed_file}")
                        # Now process the remainder (expected header + data rows)
                        reader = csv.reader(fh)
                        header = next(reader, None)  # Should be something like: ["cath_id", "count"]
                        if header is None:
                            continue
                        for row in reader:
                            if len(row) == 2:
                                cath_id, count = row
                                try:
                                    cath_counts[cath_id] += int(count)
                                except ValueError:
                                    logging.warning(f"Invalid count for cath_id '{cath_id}' in {parsed_file}")
                except Exception as e:
                    logging.error(f"Error processing {parsed_file}: {e}")
            
            if plDDT_values:
                overall_mean = statistics.mean(plDDT_values)
                overall_std = statistics.stdev(plDDT_values) if len(plDDT_values) > 1 else 0.0
            else:
                overall_mean, overall_std = 0.0, 0.0
            
            return overall_mean, overall_std, cath_counts

        def write_aggregated_results(aggregated_data):
            """
            Write a combined plDDT means file (one row per organism) and for each organism
            write a separate CATH summary file.
            
            aggregated_data is a dict where keys are organism names and values are tuples:
              (overall_mean, overall_std, cath_counts)
            """
            res_dir = "/mnt/results"
            os.makedirs(res_dir, exist_ok=True)
            
            # Write combined plDDT means file
            plddt_means_file = os.path.join(res_dir, "plDDT_means.csv")
            try:
                with open(plddt_means_file, "w", newline="", encoding="utf-8") as fh:
                    writer = csv.DictWriter(fh, fieldnames=["Organism", "Mean_plDDT", "StdDev_plDDT"])
                    writer.writeheader()
                    for organism, (mean_val, std_val, _) in sorted(aggregated_data.items()):
                        writer.writerow({
                            "Organism": organism.capitalize(),
                            "Mean_plDDT": f"{mean_val:.4f}",
                            "StdDev_plDDT": f"{std_val:.4f}"
                        })
                logging.info(f"Updated {plddt_means_file} with aggregated plDDT results.")
            except Exception as e:
                logging.error(f"Error writing {plddt_means_file}: {e}")
            
            # Write separate CATH summary CSV file for each organism
            for organism, (_, _, cath_counts) in aggregated_data.items():
                cath_summary_file = os.path.join(res_dir, f"{organism}_cath_summary.csv")
                try:
                    with open(cath_summary_file, "w", newline="", encoding="utf-8") as fh:
                        writer = csv.DictWriter(fh, fieldnames=["cath_id", "count"])
                        writer.writeheader()
                        for cath_id, count in sorted(cath_counts.items()):
                            writer.writerow({"cath_id": cath_id, "count": count})
                    logging.info(f"Wrote CATH summary to {cath_summary_file}")
                except Exception as e:
                    logging.error(f"Error writing {cath_summary_file}: {e}")

        def main():
            # Expect the aggregator to be called with a single argument: the output root directory
            # in which each organism has its own subdirectory (e.g., /mnt/results/human and /mnt/results/ecoli).
            if len(sys.argv) != 2:
                print("Usage: aggregator_after_all.py <OUTPUT_ROOT_DIR>")
                sys.exit(1)
            
            output_root = sys.argv[1]
            logging.info(f"Aggregator check started using parsed files in {output_root}...")
            
            if celery_has_active_tasks():
                logging.info("Celery tasks are still active. Skipping aggregator this hour.")
            else:
                aggregated_data = {}
                for organism in ORGANISMS:
                    organism_dir = os.path.join(output_root, organism)
                    if not os.path.isdir(organism_dir):
                        logging.warning(f"Directory for organism '{organism}' not found at {organism_dir}; skipping.")
                        continue
                    mean_val, std_val, cath_counts = aggregate_parsed_files(organism_dir, organism)
                    aggregated_data[organism] = (mean_val, std_val, cath_counts)
                
                write_aggregated_results(aggregated_data)
                logging.info("Aggregator finished.")

        if __name__ == "__main__":
            main()

         



    - name: Deploy aggregator_after_all.service
      when: "'worker1' in inventory_hostname"
      copy:
        dest: /etc/systemd/system/aggregator_after_all.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Aggregator After All Celery Tasks (Check Once)
          After=network.target

          [Service]
          Type=oneshot
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/opt/merizo_search/merizosearch_env/bin/python3 /opt/data_pipeline/aggregator_after_all.py /mnt/results/human/ human

          [Install]
          WantedBy=multi-user.target

    - name: Deploy aggregator_after_all.timer
      when: "'worker1' in inventory_hostname"
      copy:
        dest: /etc/systemd/system/aggregator_after_all.timer
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Hourly check for aggregator_after_all.service

          [Timer]
          OnCalendar=hourly
          Persistent=true
          Unit=aggregator_after_all.service

          [Install]
          WantedBy=timers.target

    - name: Reload systemd daemon (Aggregator)
      when: "'worker1' in inventory_hostname"
      systemd:
        daemon_reload: yes

    - name: Enable and start aggregator_after_all.timer
      when: "'worker1' in inventory_hostname"
      systemd:
        name: aggregator_after_all.timer
        state: started
        enabled: yes




- name: Setup Celery on Management Node and Configure Dispatch Services for Human and Ecoli
  hosts: host
  become: yes
  vars:
    redis_host: "{{ hostvars['storage']['ansible_host'] }}"
    celery_user: "almalinux"
    celery_group: "almalinux"
    virtualenv_path: "/opt/merizo_search/merizosearch_env"
    dispatch_script: "/opt/data_pipeline/dispatch_tasks.py"
    worker_queues:
      worker1: "worker1_queue"
      worker2: "worker2_queue"
      worker3: "worker3_queue"
    datasets:
      - organism: "human"
        data_input_dir: "/mnt/datasets/human_proteome/"
        results_dir: "/mnt/results/human/"
      - organism: "ecoli"
        data_input_dir: "/mnt/datasets/ecoli_proteome/"
        results_dir: "/mnt/results/ecoli/"
  tasks:
    - name: Install Celery and Redis Python packages globally
      pip:
        name:
          - celery
          - redis
        state: present

    - name: Create data pipeline directory on host
      file:
        path: /opt/data_pipeline/
        state: directory
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'

    - name: Ensure dispatch_tasks.log is writable
      file:
        path: /opt/data_pipeline/dispatch_tasks.log
        state: touch
        owner: almalinux
        group: almalinux
        mode: '0644'

    - name: Deploy Updated Dispatch Tasks Script on host
      copy:
        dest: /opt/data_pipeline/dispatch_tasks.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          import sys
          import redis
          from celery import Celery
          import glob
          import os
          import logging

          # Configure logging
          logging.basicConfig(
              filename='/opt/data_pipeline/dispatch_tasks.log',
              level=logging.DEBUG,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )

          redis_host = "{{ redis_host }}"
          redis_port = 6379
          redis_db = 0

          # Define worker queues with actual worker names
          WORKER_QUEUES = {
          {% for w, q in worker_queues.items() %}
              "{{ w }}": "{{ q }}"{% if not loop.last %},{% endif %}
          {% endfor %}
          }

          app = Celery('celery_worker', broker='redis://{}:{}/0'.format(redis_host, redis_port))

          def get_enabled_workers():
              r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
              disabled = r.smembers('disabled_workers')
              disabled = {d.decode('utf-8') for d in disabled}
              enabled = {w: q for w, q in WORKER_QUEUES.items() if w not in disabled}
              logging.debug(f"Disabled workers: {disabled}")
              logging.debug(f"Enabled workers: {enabled}")
              return enabled

          def main(input_dir, output_dir, organism):
              if organism not in ["human", "ecoli", "test"]:
                  print("Error: ORGANISM must be either 'human', 'ecoli', or 'test'")
                  sys.exit(1)

              # Initialize Redis connection
              r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
              dispatched_set_key = f"dispatched_tasks:{organism}"

              while True:
                  enabled_workers = get_enabled_workers()
                  if not enabled_workers:
                      print("No enabled workers available. Check CPU load or alerts.")
                      sys.exit(1)

                  pdb_files = glob.glob(os.path.join(input_dir, "*.pdb"))
                  pdb_files_to_process = [
                      f for f in pdb_files
                      if not os.path.exists(os.path.join(output_dir, f"{os.path.splitext(os.path.basename(f))[0]}.parsed"))
                      and not r.sismember(dispatched_set_key, f)
                  ][:100]  # Batch size of 100

                  if not pdb_files_to_process:
                      print(f"No new .pdb files to process for {organism}.")
                      break

                  worker_list = list(enabled_workers.items())
                  worker_count = len(worker_list)
                  task_index = 0

                  for pdb_file in pdb_files_to_process:
                      worker, queue = worker_list[task_index % worker_count]
                      result = app.send_task(
                          'celery_worker.run_pipeline',
                          args=[pdb_file, output_dir, organism],
                          queue=queue
                      )
                      logging.info(f"Task {result.id} dispatched for {pdb_file} to '{queue}' queue.")
                      print(f"Task {result.id} dispatched for {pdb_file} to '{queue}' queue.")
                      r.sadd(dispatched_set_key, pdb_file)
                      task_index += 1

          if __name__ == "__main__":
              if len(sys.argv) != 4:
                  print("Usage: python3 dispatch_tasks.py [INPUT_DIR] [OUTPUT_DIR] [ORGANISM]")
                  sys.exit(1)
              input_dir = sys.argv[1]
              output_dir = sys.argv[2]
              organism = sys.argv[3].lower()
              main(input_dir, output_dir, organism)

    - name: Create dispatch_tasks.sh Wrapper Script
      copy:
        dest: /opt/data_pipeline/dispatch_tasks.sh
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          #!/bin/bash
          source {{ virtualenv_path }}/bin/activate
          exec python3 {{ dispatch_script }} "$@"

    - name: Deploy Dispatch Tasks systemd Service Files for Each Dataset
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"
      copy:
        dest: "/etc/systemd/system/dispatch_tasks_{{ item.organism }}.service"
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Dispatch Tasks Service for {{ item.organism }}
          After=network.target
          Wants=network.target

          [Service]
          Type=simple
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/opt/data_pipeline/dispatch_tasks.sh "{{ item.data_input_dir }}" "{{ item.results_dir }}" "{{ item.organism }}"
          Restart=always
          RestartSec=5s

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start Dispatch Tasks services for Each Dataset
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"
      systemd:
        name: "dispatch_tasks_{{ item.organism }}.service"
        state: started
        enabled: yes

    - name: Deploy Redis cleanup Python script
      copy:
        dest: /usr/local/bin/redis_cleanup.py
        owner: root
        group: root
        mode: '0755'
        content: |
          #!/usr/bin/env python3
          import redis

          REDIS_HOST = "{{ redis_host }}"
          REDIS_PORT = 6379
          REDIS_DB   = 0

          r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB)
          r.delete('dispatched_tasks:human')
          r.delete('dispatched_tasks:ecoli')

    - name: Deploy redis_task_cleanup.service
      copy:
        dest: /etc/systemd/system/redis_task_cleanup.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Remove dispatched_tasks:human and dispatched_tasks:ecoli from Redis

          [Service]
          Type=oneshot
          ExecStart=/usr/bin/python3 /usr/local/bin/redis_cleanup.py

    - name: Deploy redis_task_cleanup.timer
      copy:
        dest: /etc/systemd/system/redis_task_cleanup.timer
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Timer for redis_task_cleanup.service

          [Timer]
          OnCalendar=hourly
          Persistent=true
          Unit=redis_task_cleanup.service

          [Install]
          WantedBy=timers.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start redis_task_cleanup.timer
      systemd:
        name: redis_task_cleanup.timer
        state: started
        enabled: yes


