- name: Setup Celery Workers
  hosts: workers
  become: yes
  vars:
    redis_host: "{{ hostvars['storage']['ansible_host'] }}"
    celery_user: "almalinux"
    celery_group: "almalinux"
    virtualenv_path: "/opt/merizo_search/merizosearch_env"
    celery_bin: "{{ virtualenv_path }}/bin/celery"
    worker_queues:
      worker1: "worker1_queue"
      worker2: "worker2_queue"
      worker3: "worker3_queue"
  tasks:
    - name: Install Celery and Redis Python packages in virtualenv
      pip:
        name:
          - celery
          - redis
        virtualenv: "{{ virtualenv_path }}"
        state: present

    - name: Create data pipeline directory
      file:
        path: /opt/data_pipeline/
        state: directory
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'

    - name: Deploy Celery Worker Script
      copy:
        dest: /opt/data_pipeline/celery_worker.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          import logging
          from celery import Celery
          import subprocess
          import os

          # Configure logging
          logging.basicConfig(
              filename='/opt/data_pipeline/celery_worker.log',
              level=logging.INFO,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )

          # Define the Redis broker URL
          app = Celery('celery_worker', broker='redis://{{ redis_host }}:6379/0')

          @app.task
          def run_pipeline(pdb_file, output_dir, organism):
              """
              Celery task to run the data pipeline on a specified PDB file.
              """
              print(f"Received PDB File: {pdb_file}")
              pipeline_script = "/opt/data_pipeline/pipeline_script.py"
              cmd = [
                  "/opt/merizo_search/merizosearch_env/bin/python3",
                  pipeline_script,
                  pdb_file,
                  output_dir,
                  organism
              ]
              logging.info(f"Running pipeline script: {' '.join(cmd)}")
              try:
                  process = subprocess.run(cmd, capture_output=True, text=True, check=True)
                  logging.info(f"Pipeline STDOUT: {process.stdout}")
                  logging.info(f"Pipeline STDERR: {process.stderr}")
                  return {
                      'stdout': process.stdout,
                      'stderr': process.stderr,
                      'returncode': process.returncode
                  }
              except subprocess.CalledProcessError as e:
                  logging.error(f"Pipeline encountered an error: {e.stderr}")
                  return {
                      'stdout': e.stdout,
                      'stderr': e.stderr,
                      'returncode': e.returncode
                  }

    - name: Set worker name
      set_fact:
        worker_name: "{{ inventory_hostname.split('-')[0] }}"

    - name: Create Celery Startup Shell Script
      copy:
        dest: /opt/data_pipeline/start_celery.sh
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          #!/bin/bash
          source {{ virtualenv_path }}/bin/activate
          exec {{ celery_bin }} -A celery_worker worker --loglevel=info --concurrency=4 --queues={{ worker_queues[worker_name] }} -n {{ worker_name }}

    - name: Deploy Celery Worker systemd Service File
      copy:
        dest: /etc/systemd/system/celery.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Celery Service
          After=network.target

          [Service]
          Type=simple
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/opt/data_pipeline/start_celery.sh
          Restart=always

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Restart and enable Celery service
      systemd:
        name: celery
        state: restarted
        enabled: yes

    - name: Ensure Celery worker script is executable
      file:
        path: /opt/data_pipeline/celery_worker.py
        mode: '0755'


- name: Setup Celery on Management Node and Configure Dispatch Services for Human and Ecoli
  hosts: host
  become: yes
  vars:
    redis_host: "{{ hostvars['storage']['ansible_host'] }}"
    celery_user: "almalinux"
    celery_group: "almalinux"
    virtualenv_path: "/opt/merizo_search/merizosearch_env"
    dispatch_script: "/opt/data_pipeline/dispatch_tasks.py"
    worker_queues:
      worker1: "worker1_queue"
      worker2: "worker2_queue"
      worker3: "worker3_queue"
    datasets:
      - organism: "human"
        data_input_dir: "/mnt/datasets/human_proteome/"
        results_dir: "/mnt/results/human/"
      - organism: "ecoli"
        data_input_dir: "/mnt/datasets/ecoli_proteome/"
        results_dir: "/mnt/results/ecoli/"
  tasks:
    - name: Install Celery and Redis Python packages globally
      pip:
        name:
          - celery
          - redis
        state: present

    - name: Create data pipeline directory on host
      file:
        path: /opt/data_pipeline/
        state: directory
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'

    - name: Ensure dispatch_tasks.log is writable
      file:
        path: /opt/data_pipeline/dispatch_tasks.log
        state: touch
        owner: almalinux
        group: almalinux
        mode: '0644'

    - name: Deploy Updated Dispatch Tasks Script on host
      copy:
        dest: /opt/data_pipeline/dispatch_tasks.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          import sys
          import redis
          from celery import Celery
          import glob
          import os
          import logging

          # Configure logging
          logging.basicConfig(
              filename='/opt/data_pipeline/dispatch_tasks.log',
              level=logging.DEBUG,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )

          redis_host = "{{ redis_host }}"
          redis_port = 6379
          redis_db = 0

          # Define worker queues with actual worker names
          WORKER_QUEUES = {
          {% for w, q in worker_queues.items() %}
              "{{ w }}": "{{ q }}"{% if not loop.last %},{% endif %}
          {% endfor %}
          }

          app = Celery('celery_worker', broker='redis://{}:{}/0'.format(redis_host, redis_port))

          def get_enabled_workers():
              r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
              disabled = r.smembers('disabled_workers')
              disabled = {d.decode('utf-8') for d in disabled}
              enabled = {w: q for w, q in WORKER_QUEUES.items() if w not in disabled}
              logging.debug(f"Disabled workers: {disabled}")
              logging.debug(f"Enabled workers: {enabled}")
              return enabled

          def main(input_dir, output_dir, organism):
              if organism not in ["human", "ecoli", "test"]:
                  print("Error: ORGANISM must be either 'human', 'ecoli', or 'test'")
                  sys.exit(1)

              # Initialize Redis connection
              r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
              dispatched_set_key = f"dispatched_tasks:{organism}"

              while True:
                  enabled_workers = get_enabled_workers()
                  if not enabled_workers:
                      print("No enabled workers available. Check CPU load or alerts.")
                      sys.exit(1)

                  pdb_files = glob.glob(os.path.join(input_dir, "*.pdb"))
                  pdb_files_to_process = [
                      f for f in pdb_files
                      if not os.path.exists(os.path.join(output_dir, f"{os.path.splitext(os.path.basename(f))[0]}.parsed"))
                      and not r.sismember(dispatched_set_key, f)
                  ][:100]  # Batch size of 100

                  if not pdb_files_to_process:
                      print(f"No new .pdb files to process for {organism}.")
                      break

                  worker_list = list(enabled_workers.items())
                  worker_count = len(worker_list)
                  task_index = 0

                  for pdb_file in pdb_files_to_process:
                      worker, queue = worker_list[task_index % worker_count]
                      result = app.send_task(
                          'celery_worker.run_pipeline',
                          args=[pdb_file, output_dir, organism],
                          queue=queue
                      )
                      logging.info(f"Task {result.id} dispatched for {pdb_file} to '{queue}' queue.")
                      print(f"Task {result.id} dispatched for {pdb_file} to '{queue}' queue.")
                      r.sadd(dispatched_set_key, pdb_file)
                      task_index += 1

          if __name__ == "__main__":
              if len(sys.argv) != 4:
                  print("Usage: python3 dispatch_tasks.py [INPUT_DIR] [OUTPUT_DIR] [ORGANISM]")
                  sys.exit(1)
              input_dir = sys.argv[1]
              output_dir = sys.argv[2]
              organism = sys.argv[3].lower()
              main(input_dir, output_dir, organism)

    - name: Create dispatch_tasks.sh Wrapper Script
      copy:
        dest: /opt/data_pipeline/dispatch_tasks.sh
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          #!/bin/bash
          source {{ virtualenv_path }}/bin/activate
          exec python3 {{ dispatch_script }} "$@"

    - name: Deploy Dispatch Tasks systemd Service Files for Each Dataset
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"
      copy:
        dest: "/etc/systemd/system/dispatch_tasks_{{ item.organism }}.service"
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Dispatch Tasks Service for {{ item.organism }}
          After=network.target
          Wants=network.target

          [Service]
          Type=simple
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/opt/data_pipeline/dispatch_tasks.sh "{{ item.data_input_dir }}" "{{ item.results_dir }}" "{{ item.organism }}"
          Restart=always
          RestartSec=5s

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start Dispatch Tasks services for Each Dataset
      loop: "{{ datasets }}"
      loop_control:
        label: "{{ item.organism }}"
      systemd:
        name: "dispatch_tasks_{{ item.organism }}.service"
        state: started
        enabled: yes

    - name: Deploy Redis cleanup Python script
      copy:
        dest: /usr/local/bin/redis_cleanup.py
        owner: root
        group: root
        mode: '0755'
        content: |
          #!/usr/bin/env python3
          import redis

          REDIS_HOST = "{{ redis_host }}"
          REDIS_PORT = 6379
          REDIS_DB   = 0

          r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB)
          r.delete('dispatched_tasks:human')
          r.delete('dispatched_tasks:ecoli')

    - name: Deploy redis_task_cleanup.service
      copy:
        dest: /etc/systemd/system/redis_task_cleanup.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Remove dispatched_tasks:human and dispatched_tasks:ecoli from Redis

          [Service]
          Type=oneshot
          ExecStart=/usr/bin/python3 /usr/local/bin/redis_cleanup.py

    - name: Deploy redis_task_cleanup.timer
      copy:
        dest: /etc/systemd/system/redis_task_cleanup.timer
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Timer for redis_task_cleanup.service

          [Timer]
          OnCalendar=hourly
          Persistent=true
          Unit=redis_task_cleanup.service

          [Install]
          WantedBy=timers.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Enable and start redis_task_cleanup.timer
      systemd:
        name: redis_task_cleanup.timer
        state: started
        enabled: yes

- name: Deploy aggregator_after_all Python script
  copy:
    dest: /opt/data_pipeline/aggregator_after_all.py
    owner: "{{ celery_user }}"
    group: "{{ celery_group }}"
    mode: '0755'
    content: |
      #!/usr/bin/env python3
      import subprocess
      import logging
      import sys

      LOGFILE = "/opt/data_pipeline/aggregator_after_all.log"
      logging.basicConfig(
          filename=LOGFILE,
          level=logging.INFO,
          format='%(asctime)s - %(levelname)s - %(message)s'
      )

      PYTHON_BIN = "/opt/merizo_search/merizosearch_env/bin/python3"
      PIPELINE_SCRIPT = "/opt/data_pipeline/pipeline_script.py"

      def celery_has_active_tasks() -> bool:
          try:
              cmd = [
                  PYTHON_BIN, "-m", "celery",
                  "-A", "celery_worker",
                  "inspect", "active"
              ]
              result = subprocess.run(cmd, capture_output=True, text=True, check=False)
              stdout = result.stdout.lower()
              return ("run_pipeline" in stdout)
          except Exception as e:
              logging.error(f"Failed to inspect Celery tasks: {e}")
              return True

      def run_aggregator(output_dir, organism):
          cmd = [
              PYTHON_BIN,
              PIPELINE_SCRIPT,
              "/fake/DUMMY.pdb",
              output_dir,
              organism,
              "run"
          ]
          logging.info(f"Running aggregator command: {' '.join(cmd)}")
          try:
              process = subprocess.run(cmd, capture_output=True, text=True, check=True)
              logging.info(f"Aggregator STDOUT:\n{process.stdout}")
              logging.info(f"Aggregator STDERR:\n{process.stderr}")
          except subprocess.CalledProcessError as e:
              logging.error(f"Aggregator encountered an error:\n{e.stderr}")

      def main():
          if len(sys.argv) != 3:
              print("Usage: aggregator_after_all.py <OUTPUT_DIR> <ORGANISM>")
              sys.exit(1)

          output_dir = sys.argv[1]
          organism = sys.argv[2].lower()
          logging.info(f"Aggregator check started for {organism}...")

          if celery_has_active_tasks():
              logging.info("Celery tasks are still active. Skipping aggregator this hour.")
          else:
              logging.info("No active tasks found. Running aggregator now.")
              run_aggregator(output_dir, organism)
              logging.info("Aggregator finished.")

      if __name__ == "__main__":
          main()

- name: Deploy aggregator_after_all.service
  copy:
    dest: /etc/systemd/system/aggregator_after_all.service
    owner: root
    group: root
    mode: '0644'
    content: |
      [Unit]
      Description=Aggregator After All Celery Tasks (Check Once)
      After=network.target

      [Service]
      Type=oneshot
      User={{ celery_user }}
      Group={{ celery_group }}
      WorkingDirectory=/opt/data_pipeline/
      ExecStart=/opt/merizo_search/merizosearch_env/bin/python3 /opt/data_pipeline/aggregator_after_all.py /mnt/results/human/ human

      [Install]
      WantedBy=multi-user.target

- name: Deploy aggregator_after_all.timer
  copy:
    dest: /etc/systemd/system/aggregator_after_all.timer
    owner: root
    group: root
    mode: '0644'
    content: |
      [Unit]
      Description=Hourly check for aggregator_after_all.service

      [Timer]
      OnCalendar=hourly
      Persistent=true
      Unit=aggregator_after_all.service

      [Install]
      WantedBy=timers.target

- name: Reload systemd daemon
  systemd:
    daemon_reload: yes

- name: Enable and start aggregator_after_all.timer
  systemd:
    name: aggregator_after_all.timer
    state: started
    enabled: yes
