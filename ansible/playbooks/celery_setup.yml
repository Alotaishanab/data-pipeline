- name: Setup Celery Workers
  hosts: workers
  become: yes
  vars:
    redis_host: "{{ hostvars['storage']['ansible_host'] }}"
    celery_user: "almalinux"
    celery_group: "almalinux"
    virtualenv_path: "/opt/merizo_search/merizosearch_env"
    celery_bin: "{{ virtualenv_path }}/bin/celery"
  tasks:
    - name: Install Celery and Redis Python packages in virtualenv
      pip:
        name:
          - celery
          - redis
        virtualenv: "{{ virtualenv_path }}"
        state: present

    - name: Create data pipeline directory
      file:
        path: /opt/data_pipeline/
        state: directory
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'

    - name: Deploy Celery Worker Script
      copy:
        dest: /opt/data_pipeline/celery_worker.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          import logging
          from celery import Celery
          import subprocess
          import os

          # Configure logging
          logging.basicConfig(
              filename='/opt/data_pipeline/celery_worker.log',
              level=logging.INFO,
              format='%(asctime)s - %(levelname)s - %(message)s'
          )

          # Define the Redis broker URL
          app = Celery('celery_worker', broker='redis://{{ redis_host }}:6379/0')

          @app.task
          def run_pipeline(input_dir, output_dir, organism):
              """
              Celery task to run the data pipeline on a specified input directory.
              
              :param input_dir: Path to the input .pdb file
              :param output_dir: Directory where output files will be saved
              :param organism: Organism type ('human' or 'ecoli')
              """
              pipeline_script = "/opt/data_pipeline/pipeline_script.py"
              cmd = [
                  "/opt/merizo_search/merizosearch_env/bin/python3",
                  pipeline_script,
                  input_dir,
                  output_dir,
                  organism
              ]
              logging.info(f"Running pipeline script: {' '.join(cmd)}")
              try:
                  process = subprocess.run(cmd, capture_output=True, text=True, check=True)
                  logging.info(f"Pipeline STDOUT: {process.stdout}")
                  logging.info(f"Pipeline STDERR: {process.stderr}")
                  return {
                      'stdout': process.stdout,
                      'stderr': process.stderr,
                      'returncode': process.returncode
                  }
              except subprocess.CalledProcessError as e:
                  logging.error(f"Pipeline encountered an error: {e.stderr}")
                  return {
                      'stdout': e.stdout,
                      'stderr': e.stderr,
                      'returncode': e.returncode
                  }

    - name: Create Celery Startup Shell Script
      copy:
        dest: /opt/data_pipeline/start_celery.sh
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          #!/bin/bash
          source {{ virtualenv_path }}/bin/activate
          exec {{ celery_bin }} -A celery_worker worker --loglevel=info --concurrency=4

    - name: Deploy Celery Worker systemd Service File
      copy:
        dest: /etc/systemd/system/celery.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Celery Service
          After=network.target

          [Service]
          Type=simple
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/opt/data_pipeline/start_celery.sh
          Restart=always

          [Install]
          WantedBy=multi-user.target
        force: yes

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Restart and enable Celery service
      systemd:
        name: celery
        state: restarted
        enabled: yes

    - name: Ensure Celery worker script is executable
      file:
        path: /opt/data_pipeline/celery_worker.py
        mode: '0755'

    - name: Upgrade networkx in virtualenv
      pip:
        name: networkx
        virtualenv: "{{ virtualenv_path }}"
        state: latest

- name: Setup Celery on Management Node
  hosts: host
  become: yes
  vars:
    redis_host: "{{ hostvars['storage']['ansible_host'] }}"
    celery_user: "almalinux"
    celery_group: "almalinux"
    virtualenv_path: "/opt/merizo_search/merizosearch_env"
    celery_bin: "{{ virtualenv_path }}/bin/celery"
  tasks:
    - name: Install Celery and Redis Python packages globally
      pip:
        name:
          - celery
          - redis
        state: present

    - name: Create data pipeline directory on host
      file:
        path: /opt/data_pipeline/
        state: directory
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'

    - name: Deploy Dispatch Tasks Script on host
      copy:
        dest: /opt/data_pipeline/dispatch_tasks.py
        owner: "{{ celery_user }}"
        group: "{{ celery_group }}"
        mode: '0755'
        content: |
          import sys
          import redis
          from celery import Celery
          import glob
          import os

          redis_host = "{{ redis_host }}"
          redis_port = 6379
          redis_db = 0

          app = Celery('celery_worker', broker=f'redis://{redis_host}:{redis_port}/0')

          def get_enabled_workers():
              r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
              disabled = r.smembers('disabled_workers')
              disabled = {d.decode('utf-8') for d in disabled}
              return disabled

          def main():
              if len(sys.argv) != 4:
                  print("Usage: python3 dispatch_tasks.py [INPUT_DIR] [OUTPUT_DIR] [ORGANISM]")
                  sys.exit(1)

              input_dir = sys.argv[1]
              output_dir = sys.argv[2]
              organism = sys.argv[3].lower()

              if organism not in ["human", "ecoli"]:
                  print("Error: ORGANISM must be either 'human' or 'ecoli'")
                  sys.exit(1)

              disabled_workers = get_enabled_workers()
              total_workers = 3  # Adjust if you have more workers

              if len(disabled_workers) >= total_workers:
                  print("All workers are disabled. No tasks will be dispatched.")
                  sys.exit(1)

              # Enqueue tasks to 'default' queue
              pdb_files = glob.glob(os.path.join(input_dir, "*.pdb"))
              if not pdb_files:
                  print(f"No .pdb files found to process in {input_dir}.")
                  sys.exit(0)

              for pdb_file in pdb_files:
                  result = app.send_task(
                      'celery_worker.run_pipeline',
                      args=[pdb_file, output_dir, organism],
                      queue='default'
                  )
                  print(f"Task {result.id} dispatched for {pdb_file} to 'default' queue.")

          if __name__ == "__main__":
              main()

    - name: Create Dispatch Tasks systemd service for human
      copy:
        dest: /etc/systemd/system/dispatch_tasks_human.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Dispatch Tasks Service for Human
          After=network.target

          [Service]
          Type=oneshot
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/usr/bin/python3 /opt/data_pipeline/dispatch_tasks.py /mnt/datasets/human_proteome/ /mnt/results/human/ human
          RemainAfterExit=yes

          [Install]
          WantedBy=multi-user.target

    - name: Create Dispatch Tasks systemd service for ecoli
      copy:
        dest: /etc/systemd/system/dispatch_tasks_ecoli.service
        owner: root
        group: root
        mode: '0644'
        content: |
          [Unit]
          Description=Dispatch Tasks Service for Ecoli
          After=network.target

          [Service]
          Type=oneshot
          User={{ celery_user }}
          Group={{ celery_group }}
          WorkingDirectory=/opt/data_pipeline/
          ExecStart=/usr/bin/python3 /opt/data_pipeline/dispatch_tasks.py /mnt/datasets/ecoli_proteome/ /mnt/results/ecoli/ ecoli
          RemainAfterExit=yes

          [Install]
          WantedBy=multi-user.target

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes

    - name: Start and enable Dispatch Tasks service for human
      systemd:
        name: dispatch_tasks_human
        state: started
        enabled: yes

    - name: Start and enable Dispatch Tasks service for ecoli
      systemd:
        name: dispatch_tasks_ecoli
        state: started
        enabled: yes

    - name: Ensure Dispatch Tasks script is executable
      file:
        path: /opt/data_pipeline/dispatch_tasks.py
        mode: '0755'